{"cells":[{"cell_type":"code","execution_count":1,"id":"871ed2c5","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","23/04/29 20:47:14 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n","23/04/29 20:47:14 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n","23/04/29 20:47:14 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n","23/04/29 20:47:14 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"]}],"source":["# imports\n","import re\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, udf\n","from pyspark.sql.types import StringType,ArrayType\n","from pyspark.sql.functions import explode\n","from pyspark.sql.functions import lower\n","\n","# start spark session  \n","spark = SparkSession.builder.getOrCreate()\n","\n","def Task2(File):\n","    #read XML\n","    df = spark.read.format('xml').options(rowTag='page').load(File)\n","\n","    # custom UDF to extract internal links\n","    def parser_string(string):\n","        results=[]\n","        try:\n","            for m in re.finditer(\"(\\[\\[.+?\\]\\])\",string):# links defined as [[link]] \n","                link=string[m.start()+2:m.end()-2]\n","                link=link.lower()\n","                if \"|\" in link: # take first link if separated with |\n","                    link=link.split(\"|\")[0]\n","                if ((\":\" not in link or link[0:9]==\"category:\") & (\"#\" not in link)):# links cannot contain : unless it starts with category: and links cannot contain a hashtag\n","                        results+=[link]\n","            return results\n","        except:\n","            return None\n","    parser_stringUDF = udf(lambda z:parser_string(z),ArrayType(StringType()))\n","\n","    # add new column \"links\" of data type array based on the UDF  \n","    df=df.withColumn(\"links\", parser_stringUDF(col(\"revision.text._VALUE\")))\n","\n","    # explode the internal link arrays so that every title article \n","    # will appear next to each link in a new row\n","    df=df.select(df.title,explode(df.links).alias('links'))\n","\n","    # sort both the titles and links ascendingly and print the first 10 rows\n","    df=df.sort(\"title\",\"links\")\n","    df=df.withColumn(\"title\", lower(col(\"title\")))\n","\n","    #save output data frame to tab delimited CSV\n","    df.coalesce(1).write.mode('overwrite').options(header='False', delimiter=\"\\t\") \\\n","     .csv('gs://comsys/task2_full.csv')\n","    return"]},{"cell_type":"code","execution_count":2,"id":"9b0e59f2","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["def Task3(File):\n","    # running task 2\n","    Task2(File)\n","\n","    # Reading in file created in task 2\n","    df = spark.read.format('csv').options(delimiter='\\t').load('gs://comsys/task2_full.csv/part-00000*.csv')\n","\n","    # keeping unique titles/links\n","    unique=df.dropDuplicates([\"_c0\",\"_c1\"])\n","\n","    # converting data frame to RDD\n","    rdd_unique=unique.rdd\n","    rdd_unique.cache()\n","\n","    # grouping value: links by key: title\n","    rdd_unique=rdd_unique.groupByKey()\n","    rdd_unique.cache()\n","\n","    # running page rank algorithm for 10 iterations\n","    for i in range(10):\n","        if i==0:\n","            # add initial rank of one and then calculate value:contribution as rank/# neighbors as a tuple for each key: neighbor link\n","            contribution = rdd_unique.mapValues(lambda x: (1,(x, len(x)))).flatMap(lambda x: [(i,x[1][0]/x[1][1][1]) for i in x[1][1][0]])\n","        else:\n","            #Combine outputed rank and then calculate value:contribution as rank/# neighbors as a tuple for each key: neighbor link\n","            # Note: initially coded to include all observations and not drop those that don't appear as a neighboring link\n","            # TA said to exclude these in office hours, non-excluding code commented below:\n","           \n","            ###contribution = rank.rightOuterJoin(rdd_unique.mapValues(lambda x: (x, len(x)))).flatMap(lambda x: [(i,1/x[1][1][1]) if x[1][0] == None else (i,x[1][0]/x[1][1][1]) for i in x[1][1][0]])\n","            contribution = rank.rightOuterJoin(rdd_unique.mapValues(lambda x: (x, len(x)))).flatMap(lambda x: [(i,x[1][0]/x[1][1][1]) for i in x[1][1][0] if x[1][0] != None])\n","\n","        # Sum all contributions of the pages that link to said neighbor, modify it as 0.15+0.85*contributions\n","        rank =contribution.reduceByKey(lambda x, y: x + y).mapValues(lambda x: 0.15+0.85*x)\n","\n","    #Convert output to data frame, sort by descending rank, output top 10 to CSV\n","    df=rank.toDF()\n","    df.cache()\n","    df=df.sort(col(\"_2\").desc()).limit(10)\n","    df.write.mode('overwrite').options(header='False', delimiter=\"\\t\") \\\n","     .csv('gs://comsys/task3.csv')\n","    return\n","\n","# initiate task 3\n","Task3('hdfs:/user/wiki-small.xml')\n","\n","# stop spark session\n","spark.stop()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":5}